{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitf6f61b6d739443d992db3bfd3acfcce2",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Direct Methods\n",
    "\n",
    "Direct Methods allow to obtain the exact solution in finitely many operations.\n",
    "\n",
    "The problem is with scale. \n",
    "For example, the well known Gauss elimination algorithm has $O(n^3)$ arithmetic complexity [<sup>1</sup>](#fn1).\n",
    "The real world problems may have a very large number of equations, like $(10^6 - 10^8)$.\n",
    "Think of a 3D, uniformly discretized mesh block like 1000x1000x1000 = $10^9$.\n",
    "\n",
    "Last but not least, the nonlinear systems of equations are always solved by iterative algorithms.\n",
    "\n",
    "---\n",
    "<span id=\"fn1\"> [Gauss algorithm](https://en.wikipedia.org/wiki/Gaussian_elimination#Computational_efficiency) </span>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Iterative methods \n",
    "\n",
    "## Intuitive Example - direct iteration method\n",
    "\n",
    "Suppose, that we would like to solve a simple equation\n",
    "\n",
    "$$ 10x = 500 $$\n",
    "\n",
    "Let us rewrite this equation into two equivalent forms:\n",
    "\n",
    "$$ 6x = 500 - 4x \\hspace{5em} and \\hspace{5em}  4x = 500 - 6x $$\n",
    "\n",
    "Each form can be adopted to generate an iterative algorithm (let's call them version A and B).\n",
    "\n",
    "$$ x_A^{k+1} = (500 - 4x_A^k)/6 \\hspace{5em} and \\hspace{5em}  x_B^{k+1} = (500 - 6x_B^k)/4 $$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task\n",
    "\n",
    "Write both versions of the iterative algorithm and try to solve the equations.\n",
    "\n",
    "Start with $ x^{(0)} = 37 $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                 #here we load numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import time, sys                   #and load some utilities\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_IC = 37\n",
    "\n",
    "N = 50 # number of iterations\n",
    "x_A = np.zeros(N)\n",
    "x_A[0] = x_IC\n",
    "\n",
    "x_B = np.zeros(N)\n",
    "x_B[0] = x_IC\n",
    "\n",
    "\n",
    "for i in range(N-1):\n",
    "    # x_A[i+1] = ...\n",
    "    # x_B[i+1] = ...\n",
    "\n",
    "# iterations = np.arange(N)\n",
    "# plt.plot(iterations, x_A)\n",
    "# plt.plot(iterations, x_B)"
   ]
  },
  {
   "source": [
    "### Task\n",
    "\n",
    "Modify the algorithm to solve $ 10x = 500 + sin(x) $\n",
    "\n",
    "### Question\n",
    "\n",
    "How an **intuitive** convergence criterion can be formulated?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Answer\n",
    "\n",
    "...\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# System of linear Equations\n",
    "\n",
    "Now, we will focus on iterative methods for solving system of linear equations:\n",
    "\n",
    "$$\n",
    "\\mathbb{A} \\boldsymbol{x} = \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "## Jacobi iterative Method\n",
    "\n",
    "Let us decompose the $\\mathbb{A}$ matrix into a Lower triangular matrix, Diagonal and Upper triangular matrix:\n",
    "\n",
    "$$\n",
    "\\mathbb{A} = \\mathbb{L} + \\mathbb{D} + \\mathbb{U}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "}_{\\mathbb{A}}\n",
    "= \n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "4 & 0 & 0 \\\\\n",
    "7 & 8 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "}_{\\mathbb{L}}\n",
    "+\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 5 & 0  \\\\\n",
    "0 & 0 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "}_{\\mathbb{D}}\n",
    "+\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "0 & 2 & 3 \\\\\n",
    "0 & 0 & 6 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "}_{\\mathbb{U}}\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "\n",
    "L = np.tril(A,-1)\n",
    "D = diagflat(diag(A))  # diag(A) - diagonal vector, diagflat(vector) - matrix form\n",
    "U = np.triu(A,1)\n",
    "print(f\"L = \\n{L}\")\n",
    "print(f\"D = \\n{D}\")\n",
    "print(f\"U = \\n{U}\")\n",
    "\n",
    "print(f\"D^-1 = \\n{np.linalg.inv(D)}\")\n",
    "\n"
   ]
  },
  {
   "source": [
    "The system $ \\mathbb{A} \\boldsymbol{x} = \\boldsymbol{b}$, can be rewrtitten as:\n",
    "\n",
    "$$\n",
    "\\mathbb{D} \\boldsymbol{x} = -(\\mathbb{L} + \\mathbb{U})\\boldsymbol{x}  + \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "Next, the iterative approach can be proposed as\n",
    "\n",
    "$$\n",
    "\\mathbb{D} \\boldsymbol{x}^{k+1} = -(\\mathbb{L} + \\mathbb{U})\\boldsymbol{x}^k  + \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "or in an equivalent form:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{k+1} = \n",
    "\\mathbb{D}^{-1} \\left(\\boldsymbol{b} - (\\mathbb{L} + \\mathbb{U})\\boldsymbol{x}^k \\right)\n",
    "$$\n",
    "\n",
    "Using index notation:\n",
    "\n",
    "$$\n",
    "x_{i}^{k+1}=\\frac{1}{a_{i i}}\\left(b_{i}-\\sum_{j=1}^{i-1} a_{i j} x_{j}^{k}-\\sum_{j=i+1}^{n} a_{i j} x_{j}^{k}\\right), i=1, \\ldots, n\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define the study case:\n",
    "\n",
    "# An ugly Matrix\n",
    "# A = np.array([[ 4, -1, -6, 0],\n",
    "#               [-5, -4, 10, 8],\n",
    "#               [ 0,  9,  4, -2],\n",
    "#               [ 1,  0, -7, 5]])\n",
    "\n",
    "# b = np.array([2, 21, -12, -6])\n",
    "\n",
    "# A nice Matrix \n",
    "A = np.array([[10., -1., 2., 0.],\n",
    "              [-1., 11., -1., 3.],\n",
    "              [2., -1., 10., -1.],\n",
    "              [0., 3., -1., 8.]])\n",
    "b = np.array([6.0, 25.0, -11.0, 15.0])\n",
    "\n",
    "initial_guess = np.zeros(4)\n",
    "\n",
    "alpha = 0.5 # Relaxation factor\n",
    "N = 25 # number of iterations\n",
    "\n",
    "iterations = np.arange(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jacobi(A,b,N,x0):\n",
    "    # do stuff...\n",
    "\n",
    "    residuals_norm = np.zeros(N)\n",
    "\n",
    "    # Iterate for N times\n",
    "    for i in range(N):\n",
    "        # residual = ...\n",
    "        # residuals_norm[i] = np.linalg.norm(residual)\n",
    "        # x = ...\n",
    "\n",
    "    return x, residuals_norm\n",
    "\n",
    "\n",
    "\n",
    "x_jacobi, residuals_norm_jacobi = jacobi(A,b,N,initial_guess)\n",
    "print(f\"jacobi: {x_jacobi}\")\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(f\"np.linalg.solve(A, b): {x_np}\")\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "figure, axis = plt.subplots(1, 1, figsize=(8, 6))\n",
    "axis.set_title('Residuals')\n",
    "axis.plot(iterations, residuals_norm_jacobi, label=r'residuals_norm_jacobi')\n",
    "axis.set_yscale('log')\n",
    "axis.set_xlabel('iterations')\n",
    "axis.set_ylabel('residuals_norm')\n",
    "axis.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "source": [
    "\n",
    "## Gauss-Seidel Method\n",
    "\n",
    "Observe, that variables ($x$) with lower indices ($j < i$) are already known at $k+1$ iteration, thus they can be inserted 'on the fly'  to enhance the convergence\n",
    "\n",
    "$$\n",
    "(\\mathbb{L} + \\mathbb{D}) \\boldsymbol{x} = -\\mathbb{U}\\boldsymbol{x}  + \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "Next, the iterative approach can be proposed as\n",
    "\n",
    "$$\n",
    "(\\mathbb{L} + \\mathbb{D})  \\boldsymbol{x}^{k+1} = -\\mathbb{U}\\boldsymbol{x}^k  + \\boldsymbol{b} \\Leftrightarrow \\\\\n",
    "\\Leftrightarrow \\boldsymbol{x}^{k+1} = \\mathbb{D}^{-1} \\left(-\\mathbb{L} \\boldsymbol{x}^{k+1} -\\mathbb{U}\\boldsymbol{x}^k  + \\boldsymbol{b} \\right)\n",
    "$$\n",
    "\n",
    "Using index notation:\n",
    "$$\n",
    "x_{i}^{k+1}=\\frac{1}{a_{i i}}\\left(b_{i}-\\sum_{j=1}^{i-1} a_{i j} x_{j}^{k+1}-\\sum_{j=i+1}^{n} a_{i j} x_{j}^{k}\\right), i=1, \\ldots, n\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def gauss_seidel(A, b, N, x0):\n",
    "    # do stuff...\n",
    "\n",
    "    residuals_norm = np.zeros(N)\n",
    "\n",
    "    for t in range(N):\n",
    "        # residual = ...\n",
    "        # residuals_norm[t] = np.linalg.norm(residual)\n",
    "        # do stuff...\n",
    "\n",
    "    return x, residuals_norm\n",
    "\n",
    "\n",
    "x_GS, residuals_norm_GS = gauss_seidel(A,b,N,initial_guess)\n",
    "print(f\"gauss_seidel:\\t\\t{x_GS}\")\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(f\"np.linalg.solve(A, b): {x_np}\")\n",
    "\n",
    "# print(f\"A@x= b \\n {A@x_np} = {b}\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "figure, axis = plt.subplots(1, 1, figsize=(8, 6))\n",
    "# plt.subplots_adjust(hspace=1)\n",
    "axis.set_title('Residuals')\n",
    "axis.plot(iterations, residuals_norm_jacobi, label=r'residuals_norm_jacobi')s\n",
    "axis.plot(iterations, residuals_norm_GS, label=r'residuals_norm_GS')\n",
    "axis.set_yscale('log')\n",
    "axis.set_xlabel('iterations')\n",
    "axis.set_ylabel('residuals_norm')\n",
    "axis.legend(loc=\"upper right\")\n",
    "# "
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Errors and Residuals\n",
    "\n",
    "Let us denote the exact solution by $ \\boldsymbol{x} $.\n",
    "\n",
    "### Definitions\n",
    "The error is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{e}^k = \\boldsymbol{x} - \\boldsymbol{x}^k\n",
    "$$\n",
    "\n",
    "The residuum is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{r}^k = \\boldsymbol{b} - \\boldsymbol{b}^k = \\boldsymbol{b} - \\mathbb{A} \\boldsymbol{x}^k = \\mathbb{A}(\\boldsymbol{x} - \\boldsymbol{x}^k) = \\mathbb{A} \\boldsymbol{e}^k\n",
    "\n",
    "$$\n",
    "\n",
    "Consider an iterative method:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{k+1} = \\boldsymbol{x}^k + \\boldsymbol{p}^k\n",
    "$$\n",
    "\n",
    "where $ \\boldsymbol{p}$ is a correction between iterations.\n",
    "\n",
    "Observe that if you calculate the error... and add it to the $\\boldsymbol{x}^k$ you should get an exact solution :)\n",
    "\n",
    "Let us state for a while that  $  \\boldsymbol{p} =  \\boldsymbol{e} = \\mathbb{A}^{-1}\\boldsymbol{r}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{k+1} = \\boldsymbol{x}^k + \\underbrace{\\mathbb{A}^{-1}\\boldsymbol{r}^k}_{\\boldsymbol{p}^k} = \\boldsymbol{x}^k + \\mathbb{A}^{-1}\\underbrace{(\\boldsymbol{b} - \\mathbb{A}\\boldsymbol{x}^k)}_{\\boldsymbol{r}^k} \\\\\n",
    "= \\boldsymbol{x}^k - \\mathbb{A}^{-1}\\mathbb{A}\\boldsymbol{x}^k + \\mathbb{A}^{-1}\\boldsymbol{b} \\\\\n",
    "= \\mathbb{A}^{-1}\\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "The problem is, the we do not know $\\mathbb{A}^{-1}$, and we do not want to calculate it as it is cumbersome.\n",
    "Instead, we would like to approximate it with another, **similar ** matrix, which has better numerical properties (is easy to invert).\n",
    "Such a matrix is called **preconditioner**.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Jacobi method - revisited\n",
    "\n",
    "Notice, that in the case Jacobi method, the $\\mathbb{A}$ matrix is approximated by its diagonal $\\mathbb{D}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{k+1} = \\boldsymbol{x}^k + \\boldsymbol{p}^k = \\boldsymbol{x}^k + \\underbrace{\\mathbb{D}^{-1}}_{\\sim \\mathbb{A}^{-1}}\\boldsymbol{r}^k = \\boldsymbol{x}^k + \\underbrace{\\mathbb{D}^{-1}  (\\boldsymbol{b} - \\mathbb{A}\\boldsymbol{x}^k)}_{\\boldsymbol{p}^k}\n",
    "$$\n",
    "\n",
    "Which is equivalant to the formula presented before:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{k+1} = \\underbrace{\\boldsymbol{x}^k - \\mathbb{D}^{-1} \\mathbb{D} \\boldsymbol{x}^k}_{=0} + \n",
    "\\underbrace{\\mathbb{D}^{-1} \\left(\\boldsymbol{b} - (\\mathbb{L} + \\mathbb{U})\\boldsymbol{x}^k \\right)}_{\\boldsymbol{\\hat{p}}^k}\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Relaxation\n",
    "\n",
    "Another 'trick', which helps to converge an iterative method, is to introduce a relaxation parameter, $\\alpha$.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}^{k+1} \n",
    "= (1 - \\alpha) \\boldsymbol{x}^k + \\alpha \\boldsymbol{\\hat{p}}^k \n",
    "= \\boldsymbol{x}^k + \\alpha \\underbrace{(\\boldsymbol{\\hat{p}}^k - \\boldsymbol{x}^k)}_{\\boldsymbol{p}^k} \\\\\n",
    "$$\n",
    "\n",
    "From the implementation point of view, it may be more convienient to use $\\boldsymbol{\\hat{p}} = \\boldsymbol{p}^k + \\boldsymbol{x}^k$.\n",
    "\n",
    "### Task\n",
    "\n",
    "Introduce the relaxation to the Jacobi and Gauss-Seidel method.\n",
    "\n",
    "* Jacobi: \n",
    "$$\n",
    "\\boldsymbol{\\hat{p}}^k = \\mathbb{D}^{-1} \\left(\\boldsymbol{b} - (\\mathbb{L} + \\mathbb{U})\\boldsymbol{x}^k \\right)\n",
    "$$\n",
    "\n",
    "* Gauss-Seidel:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{p}}^k = \\mathbb{D}^{-1} \\left(-\\mathbb{L} \\boldsymbol{x}^{k+1} -\\mathbb{U}\\boldsymbol{x}^k  + \\boldsymbol{b} \\right)\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_with_relaxation(A,b,N,x0,alpha):\n",
    "    # do stuff...\n",
    "\n",
    "    residuals_norm = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        # residual = ...\n",
    "        # residuals_norm[i] = np.linalg.norm(residual)\n",
    "        # do stuff...\n",
    "\n",
    "    return x, residuals_norm\n",
    "\n",
    "\n",
    "def gauss_seidel_SOR(A,b,N,x0, alpha=1):\n",
    "\n",
    "    residuals_norm = np.zeros(N)\n",
    "\n",
    "    for t in range(N):\n",
    "        # residual = ...\n",
    "        # residuals_norm[t] = np.linalg.norm(residual)\n",
    "        # do stuff...\n",
    "\n",
    "    return x, residuals_norm\n",
    "\n",
    "\n",
    "\n",
    "x_jacobi_with_relaxation, residuals_norm_jacobi_with_relaxation = jacobi_with_relaxation(A,b,N,initial_guess,alpha)\n",
    "print(f\"jacobi_relaxation {alpha}: {x_jacobi_with_relaxation}\")\n",
    "\n",
    "x_GS_SOR, residuals_norm_GS_SOR = gauss_seidel_SOR(A,b,N,initial_guess,alpha)\n",
    "print(f\"gauss_seidel_SOR {alpha}: {x_GS_SOR}\")\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(f\"np.linalg.solve(A, b): {x_np}\")\n",
    "\n",
    "# print(f\"A@x= b \\n {A@x_np} = {b}\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "figure, axis = plt.subplots(1, 1, figsize=(8, 6))\n",
    "# plt.subplots_adjust(hspace=1)\n",
    "axis.set_title('Residualse')\n",
    "axis.plot(iterations, residuals_norm_jacobi_with_relaxation, label=r'residuals_norm_jacobi_with_relaxation')\n",
    "axis.plot(iterations, residuals_norm_GS_SOR,label=r'gauss_seidel_SOR')\n",
    "axis.set_yscale('log')\n",
    "axis.set_xlabel('iterations')\n",
    "axis.set_ylabel('residuals_norm')\n",
    "axis.legend(loc=\"upper right\")\n",
    "# "
   ]
  },
  {
   "source": [
    "## Optimal relaxation rate\n",
    "\n",
    "It is immediately clear, that convergence rate depends on the $\\alpha$ coefficient.\n",
    "It would be beneficial, to tune the coefficient in each iteration:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{k+1} = \\mathbf{x}^{k} + \\alpha^k \\mathbf{p}^k\n",
    "$$\n",
    "\n",
    "How the residual evolve in each step?\n",
    "Let us multiply the formula above by $-\\mathbf{A}$, next add $\\mathbf{b}$ and use the definition of the residual.\n",
    "\n",
    "$$\n",
    "\\mathbf{r}^{k+1} = \\mathbf{r}^{k} - \\alpha^k \\mathbf{A} \\mathbf{p}^k\n",
    "$$\n",
    "\n",
    "Square of the norm of the residual is equal to:\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{r}^{k+1}\\| = (\\mathbf{r}^{k+1})^T \\mathbf{r}^{k+1} =\n",
    "(\\mathbf{r}^{k} - \\alpha^k \\mathbf{A} \\mathbf{p}^k)^T (\\mathbf{r}^{k} - \\alpha^k \\mathbf{A} \\mathbf{p}^k) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\mathbf{r}^{k})^T \\mathbf{r}^{k} -\n",
    "2 \\alpha^k (\\mathbf{r}^{k})^T \\mathbf{A} \\mathbf{p}^k +\n",
    "(\\alpha^k)^2(\\mathbf{A}\\mathbf{p}^k)^T \\mathbf{A}\\mathbf{p}^k\n",
    "$$\n",
    "\n",
    "Observe, that the square of the norm of the residual is a quadratic function of $\\alpha^k$.\n",
    "This function has a minimum, because the coefficient in front of $(\\alpha^k)^2$ is positive.\n",
    "Let us calculate the derivative with respect to $\\alpha^k$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\alpha^k} \\left( \\|\\mathbf{r}^{k+1}\\| \\right) =\n",
    "-2(\\mathbf{r}^{k})^T \\mathbf{A} \\mathbf{p}^k +\n",
    "2 \\alpha^k (\\mathbf{A} \\mathbf{p}^k)^T \\mathbf{A} \\mathbf{p}^k\n",
    "$$\n",
    "\n",
    "The minimum is when $\\frac{d}{d\\alpha^k} \\left( \\|\\mathbf{r}^{k+1}\\| \\right) = 0$, which corresponds with\n",
    "\n",
    "$$\n",
    "\\alpha^k = \\frac{(\\mathbf{r}^{k})^T \\mathbf{A} \\mathbf{p}^k}{(\\mathbf{A} \\mathbf{p}^k)^T \\mathbf{A} \\mathbf{p}^k}\n",
    "$$\n",
    "\n",
    "Scheme with such coefficient,$\\alpha^k$, is called Minimal Residual Method --- **MINRES** (**metoda najmniejszych residuów**).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel_MINRES(A,b,N,x0):\n",
    "    # do stuff...\n",
    "\n",
    "    p = np.zeros(len(x))\n",
    "    \n",
    "    residuals_norm = list()\n",
    "    convergence_criteria = 1e-6\n",
    "    iteration = 0 \n",
    "    residual = b - A@x\n",
    "    \n",
    "\n",
    "    while np.linalg.norm(residual) > convergence_criteria and iteration < N:\n",
    "    # while iteration < N: this fails when residuals --> 0\n",
    "        # residual = ...\n",
    "        residuals_norm.append(np.linalg.norm(residual))\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            # phat = ...\n",
    "            p[i] = phat - x[i]\n",
    "        \n",
    "       \n",
    "        # alpha = ...\n",
    "        # x = ...\n",
    "        iteration +=1\n",
    "        \n",
    "    return x, residuals_norm\n",
    "\n",
    "\n",
    "\n",
    "x_gauss_seidel_MINRES, residuals_norm_GS_MINRES = gauss_seidel_MINRES(A,b,N,initial_guess)\n",
    "print(f\"gauss_seidel_MINRES: {x_gauss_seidel_MINRES}\")\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(f\"np.linalg.solve(A, b): {x_np}\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "figure, axis = plt.subplots(1, 1, figsize=(8, 6))\n",
    "# plt.subplots_adjust(hspace=1)\n",
    "axis.set_title('Residuals')\n",
    "axis.plot(iterations[:len(residuals_norm_GS_MINRES)], residuals_norm_GS_MINRES, label=r'residuals_norm_GS_MINRES')\n",
    "axis.set_yscale('log')\n",
    "axis.set_xlabel('iterations')\n",
    "axis.set_ylabel('residual')\n",
    "axis.legend(loc=\"upper right\")"
   ]
  },
  {
   "source": [
    "## Questions:\n",
    "\n",
    "1. How much memory is requirement to store $\\boldsymbol{x}^{k+1}$ and $\\boldsymbol{x}^k$ between iteration in Jacobi and Gauss-Seidel method?\n",
    "\n",
    "2. How to parallelize the Jacobi and Gauss-Seidel method?\n",
    "\n",
    "3. Why the Jacobi algorithm fails in case of the *ugly* matrix?\n",
    "\n",
    "4. What is the difference between the *ugly* and the *nice* matrix?\n",
    "\n",
    "5. How the numerical routine like `solve(A,b)` the picks up the proper method?\n",
    "\n",
    "6. Are there any special algorithms to store sparse matrices?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## References:\n",
    "\n",
    "1. \"Metody Numeryczne dla studentów technik informacyjnych\", P. Tatjewski, Warszawa 2013\n",
    "2. \"Computational Fluid Dynamics - lecture notes\", J. Rokicki, Warszawa 2014\n",
    "<https://www.meil.pw.edu.pl/za/content/download/29896/156221/file/ComputationalFluidDynamics_20140910.pdf>\n",
    "3. Metody Numeryczne -laboratorium 3, Łukasz Łaniewski-Wołłk,\n",
    "<http://ccfd.github.io/courses/metnum_lab3.html>\n",
    "4. <https://www.quantstart.com/articles/Jacobi-Method-in-Python-and-NumPy/>\n",
    "5. <https://en.wikipedia.org/wiki/Successive_over-relaxation>\n",
    "\n",
    "## Historical notes\n",
    "\n",
    "* Human Computers: <https://en.wikipedia.org/wiki/Computer_(job_description)>\n",
    "\n",
    "* \"Pan raczy żartować, panie Feynman!\" Richard Feynman (Original title: „Surely You're Joking, Mr. Feynman!”: Adventures of a Curious Character, Princeton University Press, USA 1985)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}